model:
  embed_dim: 10
  num_experts: 4
  expert_mlp_layer: [256, 256]
  expert_activation: relu
  expert_dropout: 0.3
  expert_batch_norm: False

  gate_mlp_layer: [256, ]
  gate_activation: relu
  gate_dropout: 0.3
  gate_batch_norm: False

  tower_mlp_layer: [256, 128]
  tower_activation: relu
  tower_dropout: 0.3
  tower_batch_norm: False
